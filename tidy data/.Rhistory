sub.other <- sub.other[complete.cases(sub.other),]
sub.other.text <- str_c(sub.other$text, collapse=" ")
sub.other.text <- strsplit(sub.other.text, " ")
total <- length(sub.other.text[[1]])
unique <- length(unique(sub.other.text[[1]]))
share <- round(total/total.words, 2)
row <- data.frame(episode, speaker="ALL.OTHERS", total, unique, share)
combine.others <- rbind(combine.others, sub.top[,c(1,2,4,5,6)], row)
}
combine.others <- combine.others[order(combine.others$episode, combine.others$share),]
table(combine.others$speaker)
ggplot(combine.others, aes(episode, share)) + geom_bar(stat="identity", aes(fill=speaker)) + theme_classic()
max <- NULL
for(episode in unique(combine.others$episode)) {
subset <- combine.others[combine.others$episode==episode & combine.others$speaker!="ALL.OTHERS", ]
max2 <- max(subset$share)
keep <- subset[subset$share==max2, ]
max <- rbind(max, keep)
}
table(max$speaker)
h <- aggregate(total ~ speaker, ep, mean)
j <- aggregate(unique ~ speaker, ep, mean)
epcount <- NULL
for(episode in unique(ep$episode)){
sub <- ep[ep$episode==episode, ]
sub <- sub[complete.cases(sub),]
text <- str_c(sub$text, collapse=" ")
texts <- strsplit(text, " ")
total <- length(texts[[1]])
unique <- length(unique(texts[[1]]))
row <- data.frame(episode, total, unique)
epcount <- rbind(epcount, row)
}
ggplot(epcount, aes(episode, unique)) + geom_line(group=1)
library(ggplot2)
setwd("/Users/kaylinwalker/R/textmining_southpark/")
ep <- read.csv("raw data/southpark_byepisode_scripts.csv", stringsAsFactors=FALSE)
head(ep,1)
## number of episodes each person is in
presence <- as.data.frame.matrix(table(ep$speaker, ep$episode))
# eliminate non-recurring characters (> 5 appearances)
presence$total <- rowSums(presence)
presence <- presence[presence$total > 20, ] # cuts 1808 down to 102
presence$speaker <- row.names(presence)
presence <- presence[order(-presence$total), ]
presence$speaker <- factor(presence$speaker, levels=presence$speaker)
ggplot(presence, aes(speaker, total)) + geom_bar(stat="identity")
View(presence)
presence2 <- as.data.frame.matrix(table(ep$episode,ep$speaker))
sums <- colSums(presence2)
presence2 <- presence2[, sums > 50]
cumulative <- NULL
for(column in seq_along(presence2)) {
person <- presence2[ , column]
person <- data.frame(ep=row.names(presence2), present=person)
person$num <- sapply(person$ep, function (x) as.numeric(strsplit(as.character(x), " ")[[1]][1]))
person <- person[order(person$num), ]
person$cum <- cumsum(person$present)
item <- data.frame(num=person$num, present=person$present, cum=person$cum, person=colnames(presence2)[column])
cumulative <- rbind(cumulative, item)
}
ggplot(cumulative, aes(num, cum)) + geom_line(aes(group=person, color=person))
# who talks the most per episode?
ep$text <- iconv(ep$text, "ISO-8859-1", "UTF-8")
ep$text <- tolower(ep$text)
ep$text <- gsub("[[:punct:]]", "", ep$text)
for(h in seq_along(ep[,1])){
j <- strsplit(ep$text[h], " ")
ep$total[h] <- length(j[[1]])
ep$unique[h] <- length(unique(j[[1]]))
}
source('~/.active-rstudio-document', echo=TRUE)
head(ep)
speaker <- "KENNY"
subset <- ep[ep$speaker==speaker, ]
View(subset)
total <- sum(subset$total)
alltext <- str_c(subset$text, collapse=" ")
alltext <- strsplit(alltext, " ")
unique <- length(alltext[[1]])
unique <- length(unique(alltext[[1]]))
subset$avg.ratio <- round(subset$unique/subset$total, 2)
View(subset)
total <- sum(subset$total)
alltext <- str_c(subset$text, collapse=" ")
alltext <- strsplit(alltext, " ")
unique <- length(unique(alltext[[1]]))
mean(subset$avg.ratio)
hist(subset$avg.ratio)
boxplot(subset$avg.ratio)
overall <- round(unique/total, 2)
avgs <- NULL
for(speaker in unique(ep$speaker)){
subset <- ep[ep$speaker==speaker, ]
subset$avg.ratio <- round(subset$unique/subset$total, 2)
ep.avg <- mean(subset$avg.ratio)
total <- sum(subset$total)
alltext <- str_c(subset$text, collapse=" ")
alltext <- strsplit(alltext, " ")
unique <- length(unique(alltext[[1]]))
overall <- round(unique/total, 2)
row <- database(speaker, ep.avg, overall)
avgs <- rbind(avgs, row)
}
avgs <- NULL
for(speaker in unique(ep$speaker)){
subset <- ep[ep$speaker==speaker, ]
subset$avg.ratio <- round(subset$unique/subset$total, 2)
ep.avg <- mean(subset$avg.ratio)
total <- sum(subset$total)
alltext <- str_c(subset$text, collapse=" ")
alltext <- strsplit(alltext, " ")
unique <- length(unique(alltext[[1]]))
overall <- round(unique/total, 2)
row <- data.frame(speaker, ep.avg, overall)
avgs <- rbind(avgs, row)
}
avgs <- NULL
for(speaker in unique(ep$speaker)){
subset <- ep[ep$speaker==speaker, ]
subset$avg.ratio <- round(subset$unique/subset$total, 2)
ep.avg <- mean(subset$avg.ratio)
total <- sum(subset$total)
alltext <- str_c(subset$text, collapse=" ")
alltext <- strsplit(alltext, " ")
unique <- length(unique(alltext[[1]]))
overall <- round(unique/total, 2)
row <- data.frame(speaker, ep.avg, overall)
avgs <- rbind(avgs, row)
}
View(avgs)
speaker <- "KYLE"
subset <- ep[ep$speaker==speaker, ]
subset$avg.ratio <- round(subset$unique/subset$total, 2)
ep.avg <- mean(subset$avg.ratio)
total <- sum(subset$total)
alltext <- str_c(subset$text, collapse=" ")
alltext <- strsplit(alltext, " ")
unique <- length(unique(alltext[[1]]))
View(subset)
total <- sum(subset$total)
subset <- subset[complete.cases(subset), ]
alltext <- str_c(subset$text, collapse=" ")
alltext <- strsplit(alltext, " ")
unique <- length(unique(alltext[[1]]))
overall <- round(unique/total, 2)
row <- data.frame(speaker, ep.avg, overall)
avgs <- rbind(avgs, row)
avgs <- NULL
for(speaker in unique(ep$speaker)){
subset <- ep[ep$speaker==speaker, ]
subset$avg.ratio <- round(subset$unique/subset$total, 2)
ep.avg <- mean(subset$avg.ratio)
total <- sum(subset$total)
subset <- subset[complete.cases(subset), ]
alltext <- str_c(subset$text, collapse=" ")
alltext <- strsplit(alltext, " ")
unique <- length(unique(alltext[[1]]))
overall <- round(unique/total, 2)
row <- data.frame(speaker, total, ep.avg, overall)
avgs <- rbind(avgs, row)
}
View(avgs)
avgs <- NULL
for(speaker in unique(ep$speaker)){
subset <- ep[ep$speaker==speaker, ]
subset$avg.ratio <- round(subset$unique/subset$total, 2)
ep.avg <- mean(subset$avg.ratio)
total <- sum(subset$total)
if(total > 1000) {
subset <- subset[complete.cases(subset), ]
alltext <- str_c(subset$text, collapse=" ")
alltext <- strsplit(alltext, " ")
unique <- length(unique(alltext[[1]]))
overall <- round(unique/total, 2)
row <- data.frame(speaker, total, ep.avg, overall)
avgs <- rbind(avgs, row)
}
}
View(avgs)
avgs <- NULL
for(speaker in unique(ep$speaker)){
subset <- ep[ep$speaker==speaker, ]
subset$avg.ratio <- round(subset$unique/subset$total, 2)
ep.avg <- round(mean(subset$avg.ratio),4)
total <- sum(subset$total)
if(total > 4000) {
subset <- subset[complete.cases(subset), ]
alltext <- str_c(subset$text, collapse=" ")
alltext <- strsplit(alltext, " ")
unique <- length(unique(alltext[[1]]))
overall <- round(unique/total, 4)
row <- data.frame(speaker, total, ep.avg, overall)
avgs <- rbind(avgs, row)
}
}
View(avgs)
avgs <- NULL
for(speaker in unique(ep$speaker)){
subset <- ep[ep$speaker==speaker, ]
subset$avg.ratio <- round(subset$unique/subset$total, 2)
ep.avg <- round(mean(subset$avg.ratio),4)
total <- sum(subset$total)
if(total > 4000) {
subset <- subset[complete.cases(subset), ]
alltext <- str_c(subset$text, collapse=" ")
alltext <- strsplit(alltext, " ")
unique <- length(unique(alltext[[1]]))
overall <- round(unique/total, 4)
row <- data.frame(speaker, total, unique, ep.avg, overall)
avgs <- rbind(avgs, row)
}
}
View(avgs)
ggplot(avgs, aes(total, ep.avg)) + geom_point(aes(color=speaker))
ggplot(avgs, aes(total, overall)) + geom_point(aes(color=speaker))
ggplot(avgs, aes(ep.avg, overall)) + geom_point(aes(color=speaker))
ggplot(avgs, aes(ep.avg, overall)) + geom_point(aes(color=speaker, size=total))
subset <- ep[ep$speaker==speaker, ]
View(subset)
subset$avg.ratio <- round(subset$unique/subset$total, 2)
ep.avg <- round(mean(subset$avg.ratio),4)
total <- sum(subset$total)
subset <- subset[complete.cases(subset), ]
alltext <- str_c(subset$text, collapse=" ")
alltext <- strsplit(alltext, " ")
unique <- length(unique(alltext[[1]]))
overall <- round(unique/total, 4)
View(subset)
speaker <- "WENDY"
subset <- ep[ep$speaker==speaker, ]
subset$avg.ratio <- round(subset$unique/subset$total, 2)
ep.avg <- round(mean(subset$avg.ratio),4)
total <- sum(subset$total)
subset <- subset[complete.cases(subset), ]
alltext <- str_c(subset$text, collapse=" ")
alltext <- strsplit(alltext, " ")
unique <- length(unique(alltext[[1]]))
overall <- round(unique/total, 4)
row <- data.frame(speaker, total, unique, ep.avg, overall)
View(subset)
subset <- ep[ep$speaker==speaker, ]
subset$avg.ratio <- round(subset$unique/subset$total, 2)
subset <- subset[subset$total > 100, ]
View(subset)
ep.avg <- round(mean(subset$avg.ratio),4)
avgs <- NULL
allscores <- NULL
for(speaker in unique(ep$speaker)){
subset <- ep[ep$speaker==speaker, ]
subset$avg.ratio <- round(subset$unique/subset$total, 2)
subset <- subset[subset$total > 100, ]
ep.avg <- round(mean(subset$avg.ratio),4)
total <- sum(subset$total)
if(total > 4000) {
subset <- subset[complete.cases(subset), ]
alltext <- str_c(subset$text, collapse=" ")
alltext <- strsplit(alltext, " ")
unique <- length(unique(alltext[[1]]))
overall <- round(unique/total, 4)
row <- data.frame(speaker, total, unique, ep.avg, overall)
avgs <- rbind(avgs, row)
}
}
View(avgs)
library(tm)
library(RWeka)
library(stringr)
setwd("/Users/kaylinwalker/R/textmining_southpark/raw data")
by_person <- read.csv("southpark_byperson_scripts.csv", stringsAsFactors=FALSE)
# keep the speakers with the most words, this keeps 23
by_person2 <- by_person[nchar(by_person$text) > 10000, ]
# save the rest of the text into one big speaker "ALL.OTHERS"
kept.speakers <- unique(by_person2$speaker)
other.text <- by_person[!(by_person$speaker %in% kept.speakers), ]
other.text <- str_c(other.text$text, collapse=" ")
other <- data.frame(speaker="ALL.OTHERS", text=other.text)
# add it back in
by_person2 <- rbind(by_person2, other); rm(by_person)
# create corpus
myReader <- readTabular(mapping=list(content="text", id="speaker"))
corpus <- Corpus(DataframeSource(by_person2), readerControl=list(reader=myReader))
# pre-process text
corpus <- tm_map(corpus,content_transformer(function(x) iconv(x, to='UTF-8-MAC', sub='byte')), mc.cores=1)
corpus <- tm_map(corpus, content_transformer(tolower))
corpus <- tm_map(corpus, content_transformer(removePunctuation), mc.cores=1)
corpus <- tm_map(corpus, content_transformer(removeNumbers))
corpus <- tm_map(corpus, content_transformer(stripWhitespace))
corpus.stop.gone <- tm_map(corpus, removeWords, stopwords("english"))
library(tm)
install.packages("tm")
install.packages("RWeka")
library(tm)
library(RWeka)
library(stringr)
setwd("/Users/kaylinwalker/R/textmining_southpark/raw data")
by_person <- read.csv("southpark_byperson_scripts.csv", stringsAsFactors=FALSE)
# keep the speakers with the most words, this keeps 23
by_person2 <- by_person[nchar(by_person$text) > 10000, ]
# save the rest of the text into one big speaker "ALL.OTHERS"
kept.speakers <- unique(by_person2$speaker)
other.text <- by_person[!(by_person$speaker %in% kept.speakers), ]
other.text <- str_c(other.text$text, collapse=" ")
other <- data.frame(speaker="ALL.OTHERS", text=other.text)
# add it back in
by_person2 <- rbind(by_person2, other); rm(by_person)
# create corpus
myReader <- readTabular(mapping=list(content="text", id="speaker"))
corpus <- Corpus(DataframeSource(by_person2), readerControl=list(reader=myReader))
# pre-process text
corpus <- tm_map(corpus,content_transformer(function(x) iconv(x, to='UTF-8-MAC', sub='byte')), mc.cores=1)
corpus <- tm_map(corpus, content_transformer(tolower))
corpus <- tm_map(corpus, content_transformer(removePunctuation), mc.cores=1)
corpus <- tm_map(corpus, content_transformer(removeNumbers))
corpus <- tm_map(corpus, content_transformer(stripWhitespace))
corpus.stop.gone <- tm_map(corpus, removeWords, stopwords("english"))
options(mc.cores=1)
BigramTokenizer <- function(x) NGramTokenizer(x, Weka_control(min = 2, max = 2))
bi.tdm <- TermDocumentMatrix(corpus.stop.gone, control = list(tokenize = BigramTokenizer))
# remove sparse terms
bi.tdm.80 <- removeSparseTerms(bi.tdm, 0.8)
# save as a simple data frame
count.bi.tdm <- data.frame(inspect(bi.tdm.80))
count.bi.tdm$word <- row.names(count.bi.tdm)
TrigramTokenizer <- function(x) NGramTokenizer(x, Weka_control(min = 3, max = 3))
tri.tdm <- TermDocumentMatrix(corpus.stop.gone, control = list(tokenize = TrigramTokenizer))
# remove sparse terms
tri.tdm.80 <- removeSparseTerms(tri.tdm, 0.80)
# save as a simple data frame
count.tri.tdm <- data.frame(inspect(tri.tdm.80))
count.tri.tdm$word <- row.names(count.tri.tdm)
QuadgramTokenizer <- function(x) NGramTokenizer(x, Weka_control(min = 4, max = 4))
quad.tdm <- TermDocumentMatrix(corpus.stop.gone, control = list(tokenize = QuadgramTokenizer))
# remove sparse terms
quad.tdm.80 <- removeSparseTerms(quad.tdm, 0.80)
# save as a simple data frame
count.quad.tdm <- data.frame(inspect(quad.tdm.80))
count.quad.tdm$word <- row.names(count.quad.tdm)
QuintgramTokenizer <- function(x) NGramTokenizer(x, Weka_control(min = 5, max = 5))
quint.tdm <- TermDocumentMatrix(corpus.stop.gone, control = list(tokenize = QuintgramTokenizer))
# remove sparse terms
quint.tdm.8 <- removeSparseTerms(quint.tdm, 0.8)
count.quint.tdm <- data.frame(inspect(quint.tdm.8))
sum(colSums(count.quint.tdm))
count.quint.tdm$word <- row.names(count.quint.tdm)
write.csv(count.bi.tdm, "southpark_bi_tdm.csv", row.names=FALSE)
write.csv(count.tri.tdm, "southpark_tri_tdm.csv", row.names=FALSE)
write.csv(count.quad.tdm, "southpark_quad_tdm.csv", row.names=FALSE)
write.csv(count.quint.tdm, "southpark_quint_tdm.csv", row.names=FALSE)
inspect(count.bi.tdm[,1:10])
inspect(bi.tdm.80[,1:10])
inspect(bi.tdm.80[1:10,1:10])
library(tm)
library(stringr)
setwd("/Users/kaylinwalker/R/textmining_southpark/tidy data/tdm/")
count.tdm <- read.csv("southpark_tdm.csv", stringsAsFactors=FALSE)
count.bi.ws.tdm <- read.csv("southpark_bi_tdm.csv", stringsAsFactors=FALSE)
count.tri.ws.tdm <- read.csv("southpark_tri_tdm.csv", stringsAsFactors=FALSE)
count.quad.ws.tdm <- read.csv("southpark_quad_tdm.csv", stringsAsFactors=FALSE)
count.quint.ws.tdm <- read.csv("southpark_quint_tdm.csv", stringsAsFactors=FALSE)
LL.all <- function(df, speaker) {
LL.df <- NULL
for(word in seq_along(df[,1])) {
word <- df$word[word]
speaker.sums <- data.frame(speaker = names(colSums(df[,1:24])) , total=colSums(df[,1:24]), row.names=NULL)
word.sums <- data.frame(word = df$word , total=rowSums(df[ ,1:24]), row.names=NULL)
all.words.total <- sum(speaker.sums$total)
word.total <- word.sums[word.sums$word==word, 2]
speaker.total <- speaker.sums[speaker.sums$speaker==speaker, 2]
other.total <- all.words.total - speaker.total
speaker.word <- df[df$word==word, ]
speaker.word <- data.frame(speaker=names(speaker.word), count=t(speaker.word), row.names=NULL)
speaker.word <- as.numeric(as.character(speaker.word[speaker.word$speaker==speaker, 2]))
other.word <- word.total - speaker.word
if(speaker.word == 0) speaker.word <- 0.0001
E1 <- (speaker.total*word.total)/all.words.total
E2 <- (other.total*word.total)/all.words.total
LL <- 2*(speaker.word*log(speaker.word/E1) + other.word*log(other.word/E2))
if(E1 > speaker.word) LL <- -1*LL
speaker.word <- round(speaker.word)
row <- data.frame(speaker, word, word.total, speaker.total, speaker.word, E1, E2, LL)
LL.df <- rbind(LL.df, row)
}
LL.df <- LL.df[order(-LL.df$LL), ]
return(LL.df)
}
LL_pass <- function(df, threshold) {
output <- NULL
df <- df[rowSums(df[,1:24]) > threshold, ]
people <- colnames(df[,c(1:(length(df[1,])-1))])
for(person in people) {
temp.p <- subset(df, select=person)
temp.count <- subset(df, select=word)
temp.w <- cbind(temp.p, temp.count)
temp <- LL.all(df=df, speaker=person)
output <- rbind(output, temp)
}
return(output)
}
system.time(uniLL <- LL_pass(count.tdm, 50)) # 173.1s
system.time(biLL <- LL_pass(count.bi.ws.tdm, 25)) # 618.3s
system.time(triLL <- LL_pass(count.tri.ws.tdm, 15)) # 138.2s
system.time(quadLL <- LL_pass(count.quad.ws.tdm, 10)) # 35.2s
system.time(quintLL <- LL_pass(count.quint.ws.tdm, 5)) # 23.3s
head(cuont.quint.ws.tdm)
library(tm)
library(RWeka)
library(stringr)
setwd("/Users/kaylinwalker/R/textmining_southpark/raw data")
by_person <- read.csv("southpark_byperson_scripts.csv", stringsAsFactors=FALSE)
# keep the speakers with the most words, this keeps 23
by_person2 <- by_person[nchar(by_person$text) > 10000, ]
# save the rest of the text into one big speaker "ALL.OTHERS"
kept.speakers <- unique(by_person2$speaker)
other.text <- by_person[!(by_person$speaker %in% kept.speakers), ]
other.text <- str_c(other.text$text, collapse=" ")
other <- data.frame(speaker="ALL.OTHERS", text=other.text)
# add it back in
by_person2 <- rbind(by_person2, other); rm(by_person)
# create corpus
myReader <- readTabular(mapping=list(content="text", id="speaker"))
corpus <- Corpus(DataframeSource(by_person2), readerControl=list(reader=myReader))
# pre-process text
corpus <- tm_map(corpus,content_transformer(function(x) iconv(x, to='UTF-8-MAC', sub='byte')), mc.cores=1)
corpus <- tm_map(corpus, content_transformer(tolower))
corpus <- tm_map(corpus, content_transformer(removePunctuation), mc.cores=1)
corpus <- tm_map(corpus, content_transformer(removeNumbers))
corpus <- tm_map(corpus, content_transformer(stripWhitespace))
corpus.stop.gone <- tm_map(corpus, removeWords, stopwords("english"))
QuintgramTokenizer <- function(x) NGramTokenizer(x, Weka_control(min = 5, max = 5))
quint.tdm <- TermDocumentMatrix(corpus.stop.gone, control = list(tokenize = QuintgramTokenizer))
inspect(quint.tdm[1:10,1:10])
quint.tdm.8 <- removeSparseTerms(quint.tdm, 0.8)
inspect(quint.tdm.8[1:10,1:10])
inspect(quint.tdm.8[,1:10])
quint.tdm.8 <- removeSparseTerms(quint.tdm, 0.9)
inspect(quint.tdm.8[,1:10])
dim(quint.tdm.8)
dim(quint.tdm)
quint.tdm.8 <- removeSparseTerms(quint.tdm, 0.7)
dim(quint.tdm.8)
quint.tdm.8 <- removeSparseTerms(quint.tdm, 0.95)
dim(quint.tdm.8)
inspect(quint.tdm.8[1:10,1:10])
inspect(quint.tdm.8[1:50,1:10])
quint.tdm.8 <- removeSparseTerms(quint.tdm, 0.99)
dim(quint.tdm.8)
quint.tdm.95 <- removeSparseTerms(quint.tdm, 0.95)
dim(quint.tdm.95)
count.quint.tdm <- data.frame(inspect(quint.tdm.95))
sum(colSums(count.quint.tdm))
count.quint.tdm$word <- row.names(count.quint.tdm)
write.csv(count.quint.tdm, "southpark_quint_tdm.csv", row.names=FALSE)
count.quint.ws.tdm <- read.csv("southpark_quint_tdm.csv", stringsAsFactors=FALSE)
system.time(quintLL <- LL_pass(count.quint.ws.tdm, 5)) # 23.3s
uniLL$ngram <- 1
biLL$ngram <- 2
triLL$ngram <- 3
quadLL$ngram <- 4
quintLL$ngram <- 5
southpark_ngrams <- rbind(uniLL, biLL, triLL, quadLL, quintLL)
write.csv(southpark_ngrams, "southpark_ngrams_nostop.csv", row.names=FALSE)
View(southpark_ngrams)
ngrams <- southpark_ngrams[abs(southpark_ngrams$LL) >= 10.83, ]
n.unique <- function(df){
ngrams.unique <- NULL
words <- unique(df$word)
for(h in seq_along(words)) {
subset <- df[df$word==words[h],]
if(length(subset[,1]) > 1) subset <- subset[order(-abs(subset$LL)), ]
ngrams.unique <- rbind(ngrams.unique, subset[1,])
}
return(ngrams.unique)
}
ngrams.unique <- rbind(n.unique(ngrams[ngrams$LL >= 0, ]),
n.unique(ngrams[ngrams$LL < 0, ]))
View(ngrams.unique)
write.csv(ngrams.unique, "southpark_ngrams_filtered_nostop.csv", row.names=FALSE)
library(ggplot2)
library(RColorBrewer)
setwd("/Users/kwalker/git_projects/textmining_southpark/tidy data")
setwd("/Users/kwalker/git_projects/textmining_southpark/tidy data/")
setwd("/Users/kaylinwalker/R/textmining_southpark/tidy data")
ngrams <- read.csv("southpark_ngrams_filtered_nostop.csv", stringsAsFactors=FALSE)
main.speakers <- c("CARTMAN", "STAN", "KYLE", "KENNY", "RANDY",
"BUTTERS", "MR..GARRISON", "MS..CARTMAN")
plot <- ngrams[ngrams$speaker %in% main.speakers, ]
# split by speaker, rank by log likelihood  * ngram length, keep the top 25
rankbyspeaker <- function(df, direction) {
rank <- NULL
speakers <- unique(df$speaker)
for(j in speakers) {
subset <- df[df$speaker==j,]
subset$rank <- subset$LL*subset$ngram
subset <- subset[order(-subset$rank),]
if(length(subset[,1]) > 25) subset <- subset[1:25,]
row.names(subset) <- NULL
subset$rank2 <- as.numeric(row.names(subset))
rank <- rbind(rank, subset)
}
return(rank)
}
ranked <- rankbyspeaker(plot)
ggplot(ranked, aes(speaker, (rank2*-1))) +
geom_point(color="white") +
geom_label(aes(label=ranked$word,fill=ranked$speaker), color='white', fontface='bold') +
scale_fill_brewer(palette="Paired") +
theme_classic() +
theme(legend.position=1,plot.title = element_text(size=22)) +
labs(title="Most Characteristic Words & Phrases by South Park Character") +
xlab("") + ylab("Ranking") +
scale_y_continuous(breaks = c(-20, -10, -1), labels = c("#20", "#10", "#1"))
